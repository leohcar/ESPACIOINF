{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohcar/ESPACIOINF/blob/master/Aula_5_Exerc%C3%ADcio_Carlos_Ancasi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdORg7oe68oq",
        "outputId": "1a38bbd7-0ce5-4c0b-d9cf-c2d8a8f24709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Carlos Leonardo Ancasi Hinostroza\n"
          ]
        }
      ],
      "source": [
        "nome = \"Carlos Leonardo Ancasi Hinostroza\"\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkfGTqMVQT1u"
      },
      "source": [
        "Este exercicío consiste em treinar no MNIST um modelo de umas camadas, sendo a primeira uma camada convolucional e a segunda uma camada linear de classificação.\n",
        "\n",
        "Não podemos usar as funções torch.nn.Conv{1,2,3}d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNf4RPxQT1w"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "-fLUSHaCQT1x"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Po22b5ykhK"
      },
      "source": [
        "## Fixando as seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-7WWWgLyoRq",
        "outputId": "42938619-9965-4f38-ee22-2fae3cdf91bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb66af18ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzurMVpHxcNG"
      },
      "source": [
        "## Define pesos iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "9a6jQJLLlfF3"
      },
      "outputs": [],
      "source": [
        "in_channels = 1\n",
        "out_channels = 2\n",
        "kernel_size = 5\n",
        "stride = 3\n",
        "\n",
        "# Input image size\n",
        "height_in = 28  \n",
        "width_in = 28\n",
        "\n",
        "# Image size after the first convolutional layer.\n",
        "height_out = (height_in - kernel_size) // stride + 1\n",
        "width_out = (width_in - kernel_size) // stride + 1\n",
        "\n",
        "initial_conv_weight = torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01)\n",
        "initial_conv_bias = torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01)\n",
        "\n",
        "initial_classification_weight = torch.FloatTensor(10, out_channels * height_out * width_out).uniform_(-0.01, 0.01)\n",
        "initial_classification_bias = torch.FloatTensor(10,).uniform_(-0.01, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEMUsfJpQT11"
      },
      "source": [
        "## Dataset e dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoQjDs_QT12"
      },
      "source": [
        "### Definição do tamanho do minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "tEQYUr4TQT13"
      },
      "outputs": [],
      "source": [
        "batch_size = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7Rv_2BQT16"
      },
      "source": [
        "### Carregamento, criação dataset e do dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0dEKCn-QT17",
        "outputId": "ca98c593-e175-413c-fa68-099a3343365c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 28, 28])\n",
            "torch.Size([60000])\n"
          ]
        }
      ],
      "source": [
        "dataset_dir = '../data/'\n",
        "\n",
        "dataset_train_full = MNIST(dataset_dir, train=True, download=True,\n",
        "                           transform=torchvision.transforms.ToTensor())\n",
        "print(dataset_train_full.data.shape)\n",
        "print(dataset_train_full.targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rOy9ntrQT2D"
      },
      "source": [
        "### Usando apenas 1000 amostras do MNIST\n",
        "\n",
        "Neste exercício utilizaremos 1000 amostras de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "WNF2XjLBWWe7"
      },
      "outputs": [],
      "source": [
        "indices = torch.randperm(len(dataset_train_full))[:1000]\n",
        "dataset_train = torch.utils.data.Subset(dataset_train_full, indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYqj_oeSliYj"
      },
      "source": [
        "## Define os pesos iniciais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "aSNLD2JyA2e-"
      },
      "outputs": [],
      "source": [
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w52KGYlIQT2A",
        "outputId": "8df9d2e9-7586-42d8-9c94-f1dd9e55a446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de minibatches de trenamento: 20\n",
            "\n",
            "Dimensões dos dados de um minibatch: torch.Size([50, 1, 28, 28])\n",
            "Valores mínimo e máximo dos pixels:  tensor(0.) tensor(1.)\n",
            "Tipo dos dados das imagens:          <class 'torch.Tensor'>\n",
            "Tipo das classes das imagens:        <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "print('Número de minibatches de trenamento:', len(loader_train))\n",
        "\n",
        "x_train, y_train = next(iter(loader_train))\n",
        "print(\"\\nDimensões dos dados de um minibatch:\", x_train.size())\n",
        "print(\"Valores mínimo e máximo dos pixels: \", torch.min(x_train), torch.max(x_train))\n",
        "print(\"Tipo dos dados das imagens:         \", type(x_train))\n",
        "print(\"Tipo das classes das imagens:       \", type(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfU_v7aPfq40"
      },
      "source": [
        "## Camada Convolucional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "wRtpLJSFfsf8"
      },
      "outputs": [],
      "source": [
        "class MyConv2d(torch.nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(MyConv2d, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size  # The same for height and width.\n",
        "        self.stride = stride  # The same for height and width.\n",
        "        self.weight = torch.nn.Parameter(torch.FloatTensor(out_channels, in_channels, kernel_size, kernel_size).uniform_(-0.01, 0.01))\n",
        "        self.bias = torch.nn.Parameter(torch.FloatTensor(out_channels,).uniform_(-0.01, 0.01))\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 4, f'x must have 4 dimensions: {x.shape}'\n",
        "        # Escreva seu código aqui.\n",
        "        y_height = ( x.shape[3] - self.kernel_size) // self.stride + 1\n",
        "        y_width = ( x.shape[2] - self.kernel_size) // self.stride + 1\n",
        "        \n",
        "        out = torch.zeros((x.shape[0], self.out_channels, y_width, y_height))\n",
        "\n",
        "        for j in range(self.out_channels):\n",
        "            for k_o, k_i in enumerate(range(0,x.shape[3] - self.kernel_size+1,self.stride)):\n",
        "                for l_o, l_i in enumerate(range(0,x.shape[2] - self.kernel_size+1,self.stride)):\n",
        "                    window = x[:, :, l_i:l_i+self.kernel_size, k_i:k_i+self.kernel_size]\n",
        "                    out[:, j, l_o, k_o] = (self.weight[j, :, :, :] * window).sum(dim=-1).sum(dim=-1).squeeze()\n",
        "            out[:, j, :, :]+=self.bias[j]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROizI33sqE79"
      },
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1TuxWbkqMJc",
        "outputId": "f58a8f8f-6540-451a-af1a-aab9ef0f1890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 34.,  40.,  46.,  52.,  58.],\n",
            "          [ 70.,  76.,  82.,  88.,  94.],\n",
            "          [106., 112., 118., 124., 130.],\n",
            "          [142., 148., 154., 160., 166.]]]], grad_fn=<CopySlices>)\n",
            "tensor([[[[ 34.,  40.,  46.,  52.,  58.],\n",
            "          [ 70.,  76.,  82.,  88.,  94.],\n",
            "          [106., 112., 118., 124., 130.],\n",
            "          [142., 148., 154., 160., 166.]]]], grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "in_channels_dummy = 1\n",
        "out_channels_dummy = 1\n",
        "kernel_size_dummy = 2\n",
        "stride_dummy = 1\n",
        "\n",
        "conv_layer = MyConv2d(in_channels=in_channels_dummy, out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy)\n",
        "pytorch_conv_layer = torch.nn.Conv2d(in_channels=in_channels_dummy, out_channels=out_channels_dummy, kernel_size=kernel_size_dummy, stride=stride_dummy, padding=0)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "initial_weights_dummy = torch.arange(in_channels_dummy * out_channels_dummy * kernel_size_dummy * kernel_size_dummy).float()\n",
        "initial_weights_dummy = initial_weights_dummy.reshape(out_channels_dummy, in_channels_dummy, kernel_size_dummy, kernel_size_dummy)\n",
        "initial_bias_dummy = torch.arange(out_channels_dummy,).float()\n",
        "\n",
        "conv_layer.weight.data = initial_weights_dummy\n",
        "conv_layer.bias.data = initial_bias_dummy\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_weights_dummy, bias=initial_bias_dummy))\n",
        "\n",
        "x = torch.arange(30).float().reshape(1, 1, 5, 6)\n",
        "\n",
        "out = conv_layer(x)\n",
        "target_out = pytorch_conv_layer(x)\n",
        "print(out)\n",
        "print(target_out)\n",
        "\n",
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75UnRhdd_MW"
      },
      "source": [
        "## Compare se sua implementação está igual à do pytorch usando um exemplo aleatório"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzIjuGpWlbIM",
        "outputId": "35eec759-3039-432b-aaed-5296a11488c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ 1.2071e-02, -8.3851e-04, -1.5928e-02, -1.9509e-02, -8.5948e-03,\n",
            "           -1.5115e-02, -6.6803e-03, -1.6814e-02],\n",
            "          [-1.7955e-02,  1.0629e-03, -5.5141e-03, -6.2807e-03, -9.5148e-04,\n",
            "           -1.5553e-03, -1.7987e-02, -9.8575e-03],\n",
            "          [-1.7253e-02, -2.6109e-02, -1.4435e-02,  9.3725e-03, -2.4191e-02,\n",
            "           -4.5494e-03, -3.3627e-03, -1.7109e-02],\n",
            "          [-7.3264e-03,  6.2335e-04, -1.8412e-02, -1.4395e-02, -2.5602e-03,\n",
            "           -6.1182e-03, -8.7784e-03,  2.8005e-03],\n",
            "          [-1.3584e-02, -1.4182e-02, -5.5170e-03, -1.3134e-02, -1.6426e-02,\n",
            "           -2.6349e-03, -8.1448e-03, -2.1619e-02],\n",
            "          [ 9.9927e-04, -1.6841e-02, -2.4081e-02, -1.0928e-02, -5.8465e-03,\n",
            "            4.4981e-03, -2.4654e-02, -1.2137e-02],\n",
            "          [-1.4913e-03, -9.4631e-03, -1.2505e-02, -3.1823e-02, -1.6661e-02,\n",
            "           -2.1058e-02, -3.7382e-03, -1.6153e-02],\n",
            "          [-1.9644e-02, -1.1578e-02, -4.3898e-03, -1.9880e-02, -1.6667e-03,\n",
            "            3.8722e-03, -1.5217e-02, -4.4100e-03]],\n",
            "\n",
            "         [[ 1.0908e-02,  8.4323e-03,  9.2714e-03,  1.1156e-02,  1.3162e-02,\n",
            "            1.5026e-02,  1.5094e-02,  8.8026e-03],\n",
            "          [ 3.1718e-02,  8.2000e-03,  1.2250e-02,  1.4400e-02,  7.2724e-03,\n",
            "            9.4376e-03,  5.8832e-03,  1.4082e-02],\n",
            "          [ 3.2365e-02,  2.0176e-02,  1.2637e-02,  2.6110e-02,  1.8347e-02,\n",
            "            1.7371e-02,  2.9568e-03,  8.8065e-03],\n",
            "          [ 2.5234e-02,  1.9424e-02,  2.5591e-02,  3.7052e-02,  1.5898e-02,\n",
            "            2.6158e-02,  2.5251e-02,  2.2119e-02],\n",
            "          [ 1.8740e-02,  2.2082e-02,  1.3888e-02,  3.1083e-02,  1.7202e-02,\n",
            "            1.1954e-02,  1.0216e-02,  8.3027e-03],\n",
            "          [ 2.8244e-02,  1.6336e-02,  5.4537e-03,  2.2715e-03,  2.3194e-02,\n",
            "            1.5578e-02,  1.0319e-02,  1.3188e-02],\n",
            "          [ 2.5890e-02,  1.8127e-02,  7.9926e-03,  3.2561e-03,  1.4923e-02,\n",
            "            1.4558e-02,  2.2042e-02,  1.3529e-02],\n",
            "          [ 3.0052e-02,  3.0282e-02,  7.0667e-03,  6.0098e-03,  1.6341e-02,\n",
            "            1.0488e-02,  1.5428e-02,  2.7625e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4223e-02, -1.5828e-02, -2.7825e-02, -1.0923e-02,  1.0742e-03,\n",
            "           -2.2262e-02, -9.6219e-03, -9.6639e-03],\n",
            "          [-3.9959e-03,  3.9879e-05, -6.3555e-03, -1.9704e-02, -2.9102e-03,\n",
            "            1.1552e-04, -8.3292e-03, -4.6206e-03],\n",
            "          [-8.7682e-03,  3.4658e-03,  8.0684e-03, -3.7703e-03, -3.1142e-03,\n",
            "           -2.3094e-02, -5.3790e-03, -1.7885e-02],\n",
            "          [ 2.2877e-03, -4.4178e-03,  1.4877e-03,  4.7237e-04, -1.4728e-02,\n",
            "           -2.0689e-03, -1.5096e-02, -1.8784e-02],\n",
            "          [-2.9953e-02, -4.6312e-03, -1.0709e-02, -5.0509e-03, -9.2783e-03,\n",
            "           -1.0449e-02, -1.0341e-03,  1.5528e-03],\n",
            "          [-1.9294e-02, -1.2313e-02, -1.5206e-02, -1.5759e-02,  6.1677e-03,\n",
            "           -9.3959e-03, -4.3771e-03, -2.9469e-02],\n",
            "          [-4.8269e-03, -5.7453e-03, -2.1134e-02,  7.8137e-03, -5.3996e-03,\n",
            "           -1.8705e-03, -2.0031e-02,  2.4753e-03],\n",
            "          [-8.1905e-03, -9.1691e-03,  7.8767e-03, -1.5395e-02,  2.3946e-03,\n",
            "           -4.7345e-03, -2.3549e-03, -1.1087e-02]],\n",
            "\n",
            "         [[ 2.1771e-02,  2.4263e-02,  1.9142e-02,  7.7353e-03,  2.8513e-02,\n",
            "            1.5807e-02,  2.2036e-02,  1.8398e-02],\n",
            "          [ 1.1052e-02,  1.4215e-02,  2.3324e-02,  3.9313e-02,  2.2683e-02,\n",
            "            1.4654e-02,  2.8451e-02,  1.6324e-02],\n",
            "          [ 2.5373e-02,  3.5913e-02,  3.1290e-02,  3.4126e-02,  2.6595e-02,\n",
            "            1.0009e-02,  1.7130e-02,  1.0263e-02],\n",
            "          [ 1.0261e-02,  2.2234e-02,  2.4270e-02,  3.3105e-02,  2.2300e-02,\n",
            "            1.2544e-02,  1.8412e-02,  1.7451e-02],\n",
            "          [ 1.8885e-02,  3.2533e-02,  2.5378e-02,  1.3494e-02,  2.6677e-02,\n",
            "            9.7435e-03,  1.5322e-02,  2.3337e-02],\n",
            "          [ 7.0000e-03,  2.3459e-02,  2.3510e-02,  2.2171e-02,  1.9407e-02,\n",
            "            1.5832e-02,  2.6556e-02,  9.5252e-03],\n",
            "          [ 1.2626e-02,  8.5299e-03,  8.5031e-03,  1.1935e-02,  2.3624e-02,\n",
            "            2.9215e-02,  1.1071e-02,  2.4686e-02],\n",
            "          [ 2.6445e-02,  2.4888e-02,  8.4168e-03,  1.1538e-02,  2.5010e-02,\n",
            "            1.7069e-02,  1.7349e-02,  1.2551e-02]]]], grad_fn=<CopySlices>)\n",
            "tensor([[[[ 1.2071e-02, -8.3851e-04, -1.5928e-02, -1.9509e-02, -8.5948e-03,\n",
            "           -1.5115e-02, -6.6803e-03, -1.6814e-02],\n",
            "          [-1.7955e-02,  1.0629e-03, -5.5141e-03, -6.2807e-03, -9.5148e-04,\n",
            "           -1.5553e-03, -1.7987e-02, -9.8575e-03],\n",
            "          [-1.7253e-02, -2.6109e-02, -1.4435e-02,  9.3725e-03, -2.4191e-02,\n",
            "           -4.5494e-03, -3.3627e-03, -1.7109e-02],\n",
            "          [-7.3264e-03,  6.2335e-04, -1.8412e-02, -1.4395e-02, -2.5602e-03,\n",
            "           -6.1182e-03, -8.7784e-03,  2.8005e-03],\n",
            "          [-1.3584e-02, -1.4182e-02, -5.5170e-03, -1.3134e-02, -1.6426e-02,\n",
            "           -2.6349e-03, -8.1448e-03, -2.1619e-02],\n",
            "          [ 9.9927e-04, -1.6841e-02, -2.4081e-02, -1.0928e-02, -5.8465e-03,\n",
            "            4.4981e-03, -2.4654e-02, -1.2137e-02],\n",
            "          [-1.4913e-03, -9.4631e-03, -1.2505e-02, -3.1823e-02, -1.6661e-02,\n",
            "           -2.1058e-02, -3.7382e-03, -1.6153e-02],\n",
            "          [-1.9644e-02, -1.1578e-02, -4.3898e-03, -1.9880e-02, -1.6667e-03,\n",
            "            3.8722e-03, -1.5217e-02, -4.4100e-03]],\n",
            "\n",
            "         [[ 1.0908e-02,  8.4323e-03,  9.2714e-03,  1.1156e-02,  1.3162e-02,\n",
            "            1.5026e-02,  1.5094e-02,  8.8026e-03],\n",
            "          [ 3.1718e-02,  8.2000e-03,  1.2250e-02,  1.4400e-02,  7.2724e-03,\n",
            "            9.4376e-03,  5.8832e-03,  1.4082e-02],\n",
            "          [ 3.2365e-02,  2.0176e-02,  1.2637e-02,  2.6110e-02,  1.8347e-02,\n",
            "            1.7371e-02,  2.9568e-03,  8.8065e-03],\n",
            "          [ 2.5234e-02,  1.9424e-02,  2.5591e-02,  3.7052e-02,  1.5898e-02,\n",
            "            2.6158e-02,  2.5251e-02,  2.2119e-02],\n",
            "          [ 1.8740e-02,  2.2082e-02,  1.3888e-02,  3.1083e-02,  1.7202e-02,\n",
            "            1.1954e-02,  1.0216e-02,  8.3027e-03],\n",
            "          [ 2.8244e-02,  1.6336e-02,  5.4537e-03,  2.2715e-03,  2.3194e-02,\n",
            "            1.5578e-02,  1.0319e-02,  1.3188e-02],\n",
            "          [ 2.5890e-02,  1.8127e-02,  7.9926e-03,  3.2561e-03,  1.4923e-02,\n",
            "            1.4558e-02,  2.2042e-02,  1.3529e-02],\n",
            "          [ 3.0052e-02,  3.0282e-02,  7.0667e-03,  6.0098e-03,  1.6341e-02,\n",
            "            1.0488e-02,  1.5428e-02,  2.7625e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4223e-02, -1.5828e-02, -2.7825e-02, -1.0923e-02,  1.0742e-03,\n",
            "           -2.2262e-02, -9.6219e-03, -9.6639e-03],\n",
            "          [-3.9959e-03,  3.9879e-05, -6.3555e-03, -1.9704e-02, -2.9102e-03,\n",
            "            1.1552e-04, -8.3292e-03, -4.6206e-03],\n",
            "          [-8.7682e-03,  3.4658e-03,  8.0684e-03, -3.7703e-03, -3.1142e-03,\n",
            "           -2.3094e-02, -5.3790e-03, -1.7885e-02],\n",
            "          [ 2.2877e-03, -4.4178e-03,  1.4877e-03,  4.7237e-04, -1.4728e-02,\n",
            "           -2.0689e-03, -1.5096e-02, -1.8784e-02],\n",
            "          [-2.9953e-02, -4.6312e-03, -1.0709e-02, -5.0509e-03, -9.2783e-03,\n",
            "           -1.0449e-02, -1.0341e-03,  1.5528e-03],\n",
            "          [-1.9294e-02, -1.2313e-02, -1.5206e-02, -1.5759e-02,  6.1677e-03,\n",
            "           -9.3959e-03, -4.3771e-03, -2.9469e-02],\n",
            "          [-4.8269e-03, -5.7453e-03, -2.1134e-02,  7.8137e-03, -5.3996e-03,\n",
            "           -1.8705e-03, -2.0031e-02,  2.4753e-03],\n",
            "          [-8.1905e-03, -9.1691e-03,  7.8767e-03, -1.5395e-02,  2.3946e-03,\n",
            "           -4.7345e-03, -2.3549e-03, -1.1087e-02]],\n",
            "\n",
            "         [[ 2.1771e-02,  2.4263e-02,  1.9142e-02,  7.7353e-03,  2.8513e-02,\n",
            "            1.5807e-02,  2.2036e-02,  1.8398e-02],\n",
            "          [ 1.1052e-02,  1.4215e-02,  2.3324e-02,  3.9313e-02,  2.2683e-02,\n",
            "            1.4654e-02,  2.8451e-02,  1.6324e-02],\n",
            "          [ 2.5373e-02,  3.5913e-02,  3.1290e-02,  3.4126e-02,  2.6595e-02,\n",
            "            1.0009e-02,  1.7130e-02,  1.0263e-02],\n",
            "          [ 1.0261e-02,  2.2234e-02,  2.4270e-02,  3.3105e-02,  2.2300e-02,\n",
            "            1.2544e-02,  1.8412e-02,  1.7451e-02],\n",
            "          [ 1.8885e-02,  3.2533e-02,  2.5378e-02,  1.3494e-02,  2.6677e-02,\n",
            "            9.7435e-03,  1.5322e-02,  2.3337e-02],\n",
            "          [ 7.0000e-03,  2.3459e-02,  2.3510e-02,  2.2171e-02,  1.9407e-02,\n",
            "            1.5832e-02,  2.6556e-02,  9.5252e-03],\n",
            "          [ 1.2626e-02,  8.5299e-03,  8.5031e-03,  1.1935e-02,  2.3624e-02,\n",
            "            2.9215e-02,  1.1071e-02,  2.4686e-02],\n",
            "          [ 2.6445e-02,  2.4888e-02,  8.4168e-03,  1.1538e-02,  2.5010e-02,\n",
            "            1.7069e-02,  1.7349e-02,  1.2551e-02]]]],\n",
            "       grad_fn=<ConvolutionBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(2, in_channels, height_in, width_in)\n",
        "\n",
        "conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "pytorch_conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "# Usa os mesmos pesos para minha implementação e a do pytorch\n",
        "conv_layer.weight.data = initial_conv_weight\n",
        "conv_layer.bias.data = initial_conv_bias\n",
        "pytorch_conv_layer.load_state_dict(dict(weight=initial_conv_weight, bias=initial_conv_bias))\n",
        "\n",
        "out = conv_layer(x)\n",
        "target_out = pytorch_conv_layer(x)\n",
        "print(out)\n",
        "print(target_out)\n",
        "\n",
        "assert torch.allclose(out, target_out, atol=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQA9Zg7GQT2G"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Eg4h_kQT2H"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, height_in: int, width_in: int, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv_layer = MyConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "   \n",
        "        height_out = (height_in - kernel_size) // stride + 1\n",
        "        width_out = (width_in - kernel_size) // stride + 1\n",
        "        self.classification_layer = torch.nn.Linear(out_channels * height_out * width_out, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.conv_layer(x)\n",
        "        hidden = torch.nn.functional.relu(hidden)\n",
        "        hidden = hidden.reshape(x.shape[0], -1)\n",
        "        logits = self.classification_layer(hidden)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NHQB4wGQT2K"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqs2JhJoQT2L"
      },
      "source": [
        "### Definição dos hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZuYEkn_QT2M"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "lr = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXarXeIQT2O"
      },
      "source": [
        "### Laço de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5T_jZZPQT2P"
      },
      "outputs": [],
      "source": [
        "model = Net(height_in=height_in, width_in=width_in, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
        "\n",
        "# Usa pesos iniciais pré-difinidos\n",
        "model.classification_layer.load_state_dict(dict(weight=initial_classification_weight, bias=initial_classification_bias))\n",
        "model.conv_layer.weight.data = initial_conv_weight\n",
        "model.conv_layer.bias.data = initial_conv_bias\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "epochs = []\n",
        "loss_history = []\n",
        "loss_epoch_end = []\n",
        "total_trained_samples = 0\n",
        "for i in range(n_epochs):\n",
        "    for x_train, y_train in loader_train:\n",
        "        # predict da rede\n",
        "        outputs = model(x_train)\n",
        "\n",
        "        # calcula a perda\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # zero, backpropagation, ajusta parâmetros pelo gradiente descendente\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_trained_samples += x_train.size(0)\n",
        "        epochs.append(total_trained_samples / len(dataset_train))\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "    loss_epoch_end.append(loss.item())\n",
        "    print(f'Epoch: {i:d}/{n_epochs - 1:d} Loss: {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLL-GQlKQT2Y"
      },
      "source": [
        "### Visualização usual da perda, somente no final de cada minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w38EtNxhQT2Z"
      },
      "outputs": [],
      "source": [
        "n_batches_train = len(loader_train)\n",
        "plt.plot(epochs[::n_batches_train], loss_history[::n_batches_train])\n",
        "plt.xlabel('época')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToktJu4CK94z"
      },
      "outputs": [],
      "source": [
        "loss_epoch_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiuMsjYtQT2R"
      },
      "outputs": [],
      "source": [
        "# Assert do histórico de losses\n",
        "target_loss_epoch_end = np.array([\n",
        "    2.303267478942871,\n",
        "    2.227701187133789,\n",
        "    1.0923893451690674,\n",
        "    0.5867354869842529,\n",
        "    0.5144089460372925,\n",
        "    0.45026642084121704,\n",
        "    0.4075140357017517,\n",
        "    0.37713879346847534,\n",
        "    0.3534485101699829,\n",
        "    0.3341451585292816,\n",
        "    0.3181140422821045,\n",
        "    0.30457887053489685,\n",
        "    0.29283496737480164,\n",
        "    0.2827608287334442,\n",
        "    0.2738332152366638,\n",
        "    0.2657742500305176,\n",
        "    0.2583288848400116,\n",
        "    0.25117507576942444,\n",
        "    0.24439716339111328,\n",
        "    0.23789969086647034,\n",
        "    0.23167723417282104,\n",
        "    0.22562651336193085,\n",
        "    0.21984536945819855,\n",
        "    0.2142913043498993,\n",
        "    0.20894232392311096,\n",
        "    0.203872948884964,\n",
        "    0.19903430342674255,\n",
        "    0.19439971446990967,\n",
        "    0.18994088470935822,\n",
        "    0.18563991785049438,\n",
        "    0.18147490918636322,\n",
        "    0.17744913697242737,\n",
        "    0.17347246408462524,\n",
        "    0.16947467625141144,\n",
        "    0.16547319293022156,\n",
        "    0.16150487959384918,\n",
        "    0.1574639081954956,\n",
        "    0.1534043848514557,\n",
        "    0.14926929771900177,\n",
        "    0.1452063024044037,\n",
        "    0.1412365883588791,\n",
        "    0.13712672889232635,\n",
        "    0.1331038922071457,\n",
        "    0.1291467249393463,\n",
        "    0.1251506358385086,\n",
        "    0.12116757035255432,\n",
        "    0.11731722950935364,\n",
        "    0.11364627629518509,\n",
        "    0.11001908034086227,\n",
        "    0.10655981302261353])\n",
        "\n",
        "assert np.allclose(np.array(loss_epoch_end), target_loss_epoch_end, atol=1e-6)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "Aula 5 - Exercício - Carlos_Ancasi",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "nav_menu": {
        "height": "318px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}