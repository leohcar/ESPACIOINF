{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto_resnet_encoder_decoder",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "837bdd28acca490baf947bfc3bfd3ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e893a4ca60284392bf21a9ffa6667f0f",
              "IPY_MODEL_cb85aaa848624ed4a37bd33d3c8b992d",
              "IPY_MODEL_3577be0e28514d3cac314ebd3142fcf4"
            ],
            "layout": "IPY_MODEL_b6c51b6e3fc640da978010bbd8f882fc"
          }
        },
        "e893a4ca60284392bf21a9ffa6667f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79f7451b8a34119b783bd6b03d75c66",
            "placeholder": "​",
            "style": "IPY_MODEL_0b7a3685cfdd424cae19a0759f57c5dd",
            "value": "Downloading: 100%"
          }
        },
        "cb85aaa848624ed4a37bd33d3c8b992d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_179baf4d05f14646acdd412183d12364",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0ef0caca43e48d4ad0882aa55455d55",
            "value": 29
          }
        },
        "3577be0e28514d3cac314ebd3142fcf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471a6a3e3f4847a9a37dad29816f5cb6",
            "placeholder": "​",
            "style": "IPY_MODEL_e7352279bf4f451cbad6642118cd91ec",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.00kB/s]"
          }
        },
        "b6c51b6e3fc640da978010bbd8f882fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79f7451b8a34119b783bd6b03d75c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b7a3685cfdd424cae19a0759f57c5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "179baf4d05f14646acdd412183d12364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0ef0caca43e48d4ad0882aa55455d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "471a6a3e3f4847a9a37dad29816f5cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7352279bf4f451cbad6642118cd91ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1dac739fdb42a19644763eec9b1e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11052cba37494241b1c3f41906b6c461",
              "IPY_MODEL_f019a657d9d04666a5582dccdbc1aa24",
              "IPY_MODEL_7f33c95e5f20430089ee6b6d395845e4"
            ],
            "layout": "IPY_MODEL_c59b2a90b2e044dc96dc2f19deb0455f"
          }
        },
        "11052cba37494241b1c3f41906b6c461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bf900b00b5d4ea681769a013e446f4f",
            "placeholder": "​",
            "style": "IPY_MODEL_07c94c4463fe4321bcdce71fd9174fcc",
            "value": "Downloading: 100%"
          }
        },
        "f019a657d9d04666a5582dccdbc1aa24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eba87fcdedb7425788de3f3ac4ced20e",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f3c1f2379f24373a66d6b8746b892c0",
            "value": 213450
          }
        },
        "7f33c95e5f20430089ee6b6d395845e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86c43f21176f442bb030b76320c09474",
            "placeholder": "​",
            "style": "IPY_MODEL_59c15f2978944888abfd90882147e2b4",
            "value": " 208k/208k [00:00&lt;00:00, 772kB/s]"
          }
        },
        "c59b2a90b2e044dc96dc2f19deb0455f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bf900b00b5d4ea681769a013e446f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c94c4463fe4321bcdce71fd9174fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eba87fcdedb7425788de3f3ac4ced20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3c1f2379f24373a66d6b8746b892c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86c43f21176f442bb030b76320c09474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59c15f2978944888abfd90882147e2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d885ae0e92040bd809e4fec6016b63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2c979b968ae4a16b4f95678bf8ca64a",
              "IPY_MODEL_25ac4bf34fbe4b169264abe506376d7c",
              "IPY_MODEL_250f56b994104d7ea07284d5b2be14b7"
            ],
            "layout": "IPY_MODEL_7690e54c929a432381d24fe5a5f0da4c"
          }
        },
        "c2c979b968ae4a16b4f95678bf8ca64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1894ac4933f64a05b2f66d063b3bd0f4",
            "placeholder": "​",
            "style": "IPY_MODEL_67531d57486d4fca9a85d44de7702d8c",
            "value": "Downloading: 100%"
          }
        },
        "25ac4bf34fbe4b169264abe506376d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_901aceece8a84368be94d8f13c88c212",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4211e69a11448749c9bf87f28a3030a",
            "value": 435797
          }
        },
        "250f56b994104d7ea07284d5b2be14b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f851518d9a8400d942e6d580998a09a",
            "placeholder": "​",
            "style": "IPY_MODEL_ebfe70e63b9a447893554c64e8d406f3",
            "value": " 426k/426k [00:00&lt;00:00, 924kB/s]"
          }
        },
        "7690e54c929a432381d24fe5a5f0da4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1894ac4933f64a05b2f66d063b3bd0f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67531d57486d4fca9a85d44de7702d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "901aceece8a84368be94d8f13c88c212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4211e69a11448749c9bf87f28a3030a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f851518d9a8400d942e6d580998a09a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebfe70e63b9a447893554c64e8d406f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8cc2dab6efd48b28c37ac1e568a939d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c48330778564d8abaa7cd56905a23e4",
              "IPY_MODEL_66da18e4003c4d33b3c23400236de6df",
              "IPY_MODEL_639b646bca38468aa36ce013ee73d5dd"
            ],
            "layout": "IPY_MODEL_4b3681a102ce494697f633c28a56e23d"
          }
        },
        "7c48330778564d8abaa7cd56905a23e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef0f7369b53e4df2aa88918f230e7139",
            "placeholder": "​",
            "style": "IPY_MODEL_05305d103d1b42ec8161a996942b667a",
            "value": "Downloading: 100%"
          }
        },
        "66da18e4003c4d33b3c23400236de6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aa8162ee25d4a70a7aa65f7a1e5daf2",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_535397cc87344348b31a844bf944c9da",
            "value": 570
          }
        },
        "639b646bca38468aa36ce013ee73d5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f11d6a5577846f1bb8593c52de9479e",
            "placeholder": "​",
            "style": "IPY_MODEL_c512e8e4a9a04909837197b31a838da3",
            "value": " 570/570 [00:00&lt;00:00, 24.7kB/s]"
          }
        },
        "4b3681a102ce494697f633c28a56e23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef0f7369b53e4df2aa88918f230e7139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05305d103d1b42ec8161a996942b667a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2aa8162ee25d4a70a7aa65f7a1e5daf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "535397cc87344348b31a844bf944c9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f11d6a5577846f1bb8593c52de9479e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c512e8e4a9a04909837197b31a838da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohcar/ESPACIOINF/blob/master/Proyecto_resnet_encoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Carlos Leonardo Ancasi Hinostroza'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1fd4e3-ef89-4fd9-91f1-4d3e4cea8ec8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Carlos Leonardo Ancasi Hinostroza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "# Modelo de image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "b652a9bd-a66a-41ff-bbd9-f3150acbe570"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 79.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI"
      ],
      "metadata": {
        "id": "4P-K3StTp76B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e8859c-858c-424d-b593-b745eef95d63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c1e696-5f87-442a-9d43-90b62b690e7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul  6 22:08:37 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6827c73-ecd1-450d-f5b2-09c61c024e15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Coco"
      ],
      "metadata": {
        "id": "fHGebkCBqLPb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    # Recomenda-se usar o tokenizer.batch_encode_plus pois é mais rápido.\n",
        "    return (tokenizer.batch_encode_plus([text], return_tensors=None, add_special_tokens=False).input_ids)[0]\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        # Escreva aqui seu código.\n",
        "        self.x = []\n",
        "        self.max_seq_length = max_seq_length\n",
        "        x = [101]\n",
        "        x.extend([tokenizer.pad_token_id]*self.max_seq_length)        \n",
        "\n",
        "        for texto in texts:\n",
        "            token = tokenize(texto, tokenizer)\n",
        "            for i in range(0, len(token), (self.max_seq_length - 1) ):\n",
        "                context_size = (self.max_seq_length - 1)\n",
        "                if i +  max_seq_length - 1 > len(token):\n",
        "                    context_size = len(token) % (self.max_seq_length - 1)\n",
        "                x_a = x[:]\n",
        "                x_a[1:context_size+1]=token[i:i+context_size]\n",
        "\n",
        "                self.x.append(x_a)           \n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva aqui seu código.\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva aqui seu código.\n",
        "        return torch.LongTensor(self.x[idx][:-1]), torch.LongTensor(self.x[idx][1:])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=10)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "\n",
        "#assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "print(first_batch_input)\n",
        "print(first_batch_target)\n",
        "\n",
        "# assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "# assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "837bdd28acca490baf947bfc3bfd3ff7",
            "e893a4ca60284392bf21a9ffa6667f0f",
            "cb85aaa848624ed4a37bd33d3c8b992d",
            "3577be0e28514d3cac314ebd3142fcf4",
            "b6c51b6e3fc640da978010bbd8f882fc",
            "f79f7451b8a34119b783bd6b03d75c66",
            "0b7a3685cfdd424cae19a0759f57c5dd",
            "179baf4d05f14646acdd412183d12364",
            "c0ef0caca43e48d4ad0882aa55455d55",
            "471a6a3e3f4847a9a37dad29816f5cb6",
            "e7352279bf4f451cbad6642118cd91ec",
            "2a1dac739fdb42a19644763eec9b1e1c",
            "11052cba37494241b1c3f41906b6c461",
            "f019a657d9d04666a5582dccdbc1aa24",
            "7f33c95e5f20430089ee6b6d395845e4",
            "c59b2a90b2e044dc96dc2f19deb0455f",
            "9bf900b00b5d4ea681769a013e446f4f",
            "07c94c4463fe4321bcdce71fd9174fcc",
            "eba87fcdedb7425788de3f3ac4ced20e",
            "2f3c1f2379f24373a66d6b8746b892c0",
            "86c43f21176f442bb030b76320c09474",
            "59c15f2978944888abfd90882147e2b4",
            "8d885ae0e92040bd809e4fec6016b63b",
            "c2c979b968ae4a16b4f95678bf8ca64a",
            "25ac4bf34fbe4b169264abe506376d7c",
            "250f56b994104d7ea07284d5b2be14b7",
            "7690e54c929a432381d24fe5a5f0da4c",
            "1894ac4933f64a05b2f66d063b3bd0f4",
            "67531d57486d4fca9a85d44de7702d8c",
            "901aceece8a84368be94d8f13c88c212",
            "d4211e69a11448749c9bf87f28a3030a",
            "3f851518d9a8400d942e6d580998a09a",
            "ebfe70e63b9a447893554c64e8d406f3",
            "b8cc2dab6efd48b28c37ac1e568a939d",
            "7c48330778564d8abaa7cd56905a23e4",
            "66da18e4003c4d33b3c23400236de6df",
            "639b646bca38468aa36ce013ee73d5dd",
            "4b3681a102ce494697f633c28a56e23d",
            "ef0f7369b53e4df2aa88918f230e7139",
            "05305d103d1b42ec8161a996942b667a",
            "2aa8162ee25d4a70a7aa65f7a1e5daf2",
            "535397cc87344348b31a844bf944c9da",
            "8f11d6a5577846f1bb8593c52de9479e",
            "c512e8e4a9a04909837197b31a838da3"
          ]
        },
        "outputId": "8a1b2b5e-1a6a-4581-c90d-3414b3b6928b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "837bdd28acca490baf947bfc3bfd3ff7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a1dac739fdb42a19644763eec9b1e1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d885ae0e92040bd809e4fec6016b63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8cc2dab6efd48b28c37ac1e568a939d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "tensor([[  101,   142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0],\n",
            "        [  101,  2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435],\n",
            "        [  101,  1197, 13473,     0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([[  142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0,     0],\n",
            "        [ 2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435,     0],\n",
            "        [ 1197, 13473,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset COCO para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4luQpPPdQWNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e91dddd-b09c-4458-fb6a-a8f41d383f54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./downloads\n",
        "!wget -nc --directory-prefix=downloads http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget -nc --directory-prefix=downloads http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget -nc --directory-prefix=downloads http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ],
      "metadata": {
        "id": "a6Wpq6c6BXkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502dd799-cff4-46de-e798-0810043d2206"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-06 22:17:14--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.40.145\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.40.145|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘downloads/train2017.zip’\n",
            "\n",
            "train2017.zip       100%[===================>]  18.01G  45.6MB/s    in 6m 57s  \n",
            "\n",
            "2022-07-06 22:24:11 (44.2 MB/s) - ‘downloads/train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2022-07-06 22:24:11--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.130.99\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.130.99|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘downloads/val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  46.2MB/s    in 18s     \n",
            "\n",
            "2022-07-06 22:24:29 (43.0 MB/s) - ‘downloads/val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2022-07-06 22:24:29--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.134.1\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.134.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘downloads/annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  46.4MB/s    in 5.6s    \n",
            "\n",
            "2022-07-06 22:24:35 (42.9 MB/s) - ‘downloads/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./data\n",
        "!unzip -q downloads/train2017.zip -d data/\n",
        "!unzip -q downloads/val2017.zip -d data/\n",
        "!unzip downloads/annotations_trainval2017.zip -d data/"
      ],
      "metadata": {
        "id": "026Ddq2jXBiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7722f385-c6f9-445d-d023-3153a90ad179"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  downloads/annotations_trainval2017.zip\n",
            "  inflating: data/annotations/instances_train2017.json  \n",
            "  inflating: data/annotations/instances_val2017.json  \n",
            "  inflating: data/annotations/captions_train2017.json  \n",
            "  inflating: data/annotations/captions_val2017.json  \n",
            "  inflating: data/annotations/person_keypoints_train2017.json  \n",
            "  inflating: data/annotations/person_keypoints_val2017.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm downloads/*.zip"
      ],
      "metadata": {
        "id": "52rK7vsIgho7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "max_seq_length = 12\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    # Recomenda-se usar o tokenizer.batch_encode_plus pois é mais rápido.\n",
        "    return (tokenizer.batch_encode_plus([text], return_tensors=None, add_special_tokens=False).input_ids)[0]\n",
        "\n",
        "def transform_target(texts):\n",
        "    out = []\n",
        "    x = [101]\n",
        "    x.extend([tokenizer.pad_token_id]*max_seq_length)\n",
        "\n",
        "    for texto in texts:\n",
        "        token = tokenize(texto, tokenizer)\n",
        "        for i in range(0, len(token), (max_seq_length - 1) ):\n",
        "            context_size = (max_seq_length - 1)\n",
        "            if i +  max_seq_length - 1 > len(token):\n",
        "                context_size = len(token) % (max_seq_length - 1)\n",
        "            x_a = x[:]\n",
        "            x_a[1:context_size+1]=token[i:i+context_size]\n",
        "\n",
        "            out.append(x_a)\n",
        "    return torch.LongTensor(out[:5])"
      ],
      "metadata": {
        "id": "LQpdLzamE0bX"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "a = transform_target(dummy_texts)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nHbZlWppYG",
        "outputId": "50bc0e3e-53fe-4664-e2ac-826221c7bc69"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,   142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0,\n",
              "             0,     0,     0],\n",
              "        [  101,  2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435,\n",
              "          1197, 13473,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_image = torchvision.transforms.Compose([\n",
        "                                                  torchvision.transforms.Resize([100,100]),\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "# transform_image = torchvision.transforms.Compose([\n",
        "#                                                   torchvision.transforms.ToTensor(),\n",
        "#                                                   torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "path2data_train = './data/train2017'\n",
        "path2capt_train = './data/annotations/captions_train2017.json'\n",
        "\n",
        "path2data_val = './data/val2017'\n",
        "path2capt_val = './data/annotations/captions_val2017.json'\n",
        "\n",
        "\n",
        "training_dataset = torchvision.datasets.CocoCaptions(root = path2data_train, annFile = path2capt_train, transform=transform_image, target_transform=transform_target)\n",
        "valid_dataset = torchvision.datasets.CocoCaptions(root = path2data_val, annFile = path2capt_val, transform=transform_image, target_transform=transform_target)\n"
      ],
      "metadata": {
        "id": "7VCjXViBBNFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e61ad04-28f5-4390-90ca-14c01c9f42fb"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.90s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.05s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776f9322-e1ed-48d2-ba12-1125135d896c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 118287\n",
            "valid examples: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img, sample_label = next(iter(DataLoader(training_dataset)))\n",
        "print(sample_img.shape)\n",
        "print(sample_label.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrfPCxGmrvLH",
        "outputId": "07e2c734-92e2-4218-f347-8e59c5a36881"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 100, 100])\n",
            "torch.Size([1, 5, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img, sample_label = next(iter(DataLoader(valid_dataset)))\n",
        "print(sample_img.shape)\n",
        "print(sample_label.shape)\n",
        "print(sample_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-rzIxOITT0T",
        "outputId": "d3c9398b-d6b7-4cbf-deef-f338a2f3f134"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 100, 100])\n",
            "torch.Size([1, 5, 13])\n",
            "tensor([[[ 101,  138, 1590, 4061, 1107, 1103, 7659, 1298, 1120, 1103, 1952,\n",
            "           119,    0],\n",
            "         [ 101,  138, 1395, 1114, 8391,  117,  170, 1952,  117, 1105,  170,\n",
            "          1590,    0],\n",
            "         [ 101, 1107, 1122,  119,    0,    0,    0,    0,    0,    0,    0,\n",
            "             0,    0],\n",
            "         [ 101,  138, 1590, 2288, 1107,  170, 3119, 1118,  170, 2487,    0,\n",
            "             0,    0],\n",
            "         [ 101,  138, 1825, 2288, 1120,  170, 1952, 1107,  170, 1395,  119,\n",
            "             0,    0]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model decoder"
      ],
      "metadata": {
        "id": "4QfjVNLskyrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(torch.nn.Module):\n",
        "    def __init__(self, dim, n_head):\n",
        "        \"\"\"\n",
        "        Implements the Multi head Attention\n",
        "\n",
        "        Args:\n",
        "            dim : Dimension of the embedding layer for each word in the context.\n",
        "            n_layers : number of self-attention layers.\n",
        "        \"\"\"\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        \n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "\n",
        "        # Dimension de cada head\n",
        "        self.dim_head = self.dim // self.n_head\n",
        "\n",
        "        # As matrixes W_k, W_q, W_v, W_e   \n",
        "        self.W_k = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_q = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_v = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_e = nn.Linear(self.dim, self.dim , bias=False)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, key, value, query, mask = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length, dimension)  (B,L,D)\n",
        "            mask is Tensor of shape (batch_size, 1, max_seq_length,max_seq_length) (B,1,L,L)\n",
        "        Returns:\n",
        "            LongTensor of shape (batch_size, max_seq_length, dimension)\n",
        "        \"\"\"\n",
        "        batch_size = value.size(0)\n",
        "        max_seq_length = value.size(1)\n",
        "\n",
        "        # k, q, v tem dimensão B, L, H, D/H \n",
        "        k = self.W_k(key).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "        q = self.W_q(query).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "        v = self.W_v(value).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "\n",
        "        # Transpor para B, H, L, D/H\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # atention\n",
        "        scores = torch.matmul(q, torch.transpose(k,-1,-2))   # B, H, L, L\n",
        "\n",
        "        # aplicar mascara\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, float(\"-1e16\"))\n",
        "\n",
        "        scores = scores/math.sqrt(self.dim_head)\n",
        "\n",
        "        # aplicar sofmax\n",
        "        probs = self.softmax(scores) # B, H, L, L\n",
        "\n",
        "        probs  = torch.matmul(probs, v) # B, H, L, D/H \n",
        "\n",
        "        probs = probs.transpose(1,2).contiguous() # B, L, H, D/H \n",
        "        probs = probs.reshape(batch_size,max_seq_length, self.dim) # B, L, D\n",
        "\n",
        "        return self.W_e(probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "a31ZiZEUdP9R"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, dim: int, n_head: int, expansion_factor: int):\n",
        "        \"\"\"\n",
        "        Implements Transformer Block \n",
        "        Args:\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_head (int): number of self-attention head\n",
        "        \"\"\"\n",
        "        super(TransformerBlock,self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.multi_head = MultiheadAttention(self.dim, self.n_head)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        hidden_size = self.expansion_factor * self.dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.dim)\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(self.dim)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, key, value, query, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length, dimension)  (B,L,D)\n",
        "            mask is Tensor of shape (batch_size, 1, max_seq_length,max_seq_length) (B,1,L,L)\n",
        "        Returns:\n",
        "            LongTensor of shape (batch_size, max_seq_length, dimension)\n",
        "        \"\"\"\n",
        "\n",
        "        attention = self.multi_head(key, value, query, mask=mask)\n",
        "        attention_residual = attention + query\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual))\n",
        "        \n",
        "        feed_fwd = self.feed_forward(norm1_out)\n",
        "        feed_fwd_residual = feed_fwd + norm1_out\n",
        "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual))\n",
        "        \n",
        "        return norm2_out\n"
      ],
      "metadata": {
        "id": "FA0PkKhVwjjA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self, dim: int, n_head: int, expansion_factor: int):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "        self.expasion_factor = expansion_factor\n",
        "\n",
        "        self.attention = MultiheadAttention(self.dim, self.n_head)\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.transformer_block = TransformerBlock(self.dim, self.n_head, self.expasion_factor)\n",
        "    \n",
        "    def forward(self, enc_out, x, mask_self, mask_cross=None):\n",
        "\n",
        "        attention = self.attention(x, x, x, mask_self)\n",
        "        attention_residual = attention + x\n",
        "        out = self.dropout1(self.norm1(attention_residual))\n",
        "\n",
        "        out = self.transformer_block(enc_out,enc_out,out, mask_cross)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "HCXmrfIQrYu4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int, n_head: int, expansion_factor: int):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "            n_head(int): number of head of self-attention\n",
        "            expansion_factor(int): fator for the hidden size of feed_forward\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super(TransformerDecoder,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        # C()\n",
        "        self.C_w = nn.Embedding(vocab_size, dim)\n",
        "\n",
        "        # P()\n",
        "        self.P_w = nn.Embedding(max_seq_length, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderBlock(self.dim,self.n_head,self.expansion_factor) for i in range(self.n_layers)])\n",
        "\n",
        "        self.linear_out = nn.Linear(self.dim,self.vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, enc_out=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            \n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        \n",
        "        batch_size = inputs.size(0)\n",
        "        max_seq_length = inputs.size(1)\n",
        "\n",
        "        c_emb = self.C_w(inputs)  # B,L,D\n",
        "        p_emb = self.P_w(torch.LongTensor(range(0,self.max_seq_length)).unsqueeze(0).to(inputs.device))\n",
        "\n",
        "\n",
        "        x = c_emb + p_emb  # B,L,D\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # generar mascara\n",
        "        mask_tri = (torch.ones(self.max_seq_length,self.max_seq_length).tril() == 1).expand(batch_size,1,self.max_seq_length,self.max_seq_length).to(inputs.device)\n",
        "        \n",
        "        mask_pad = (inputs != self.pad_token_id)\n",
        "        mask_pad = mask_pad.reshape(batch_size,1,1,self.max_seq_length).expand(batch_size,1,self.max_seq_length,self.max_seq_length).to(inputs.device)\n",
        "\n",
        "        mask = torch.logical_and(mask_tri,mask_pad)\n",
        "        \n",
        "        if enc_out is None:\n",
        "            enc_out = x\n",
        "            mask_cross = mask_tri\n",
        "        else:\n",
        "            mask_cross = None\n",
        "\n",
        "        for layer in self.layers:\n",
        "\n",
        "            x = layer(enc_out,x,mask_self=mask,mask_cross=mask_cross)   # B, L, D\n",
        "\n",
        "        out = self.linear_out(x) # B, L, vocab\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "aHogH_bGVpH-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model encoder"
      ],
      "metadata": {
        "id": "XOGkEGC9lBNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class MyBlock(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        identity_downsample=None,\n",
        "        stride: int = 1,\n",
        "    ) -> None:\n",
        "        super(MyBlock, self).__init__()\n",
        "        # Escreva seu código aqui.\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, bias=False, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, bias=False, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Escreva seu código aqui.\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "FRUodY_gj5cz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_resnet(nn.Module):\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "\n",
        "        super(CNN_resnet,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #resnet layers\n",
        "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
        "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
        "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
        "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "      \n",
        "    def __make_layer(self, in_channels, out_channels, stride):\n",
        "\n",
        "        identity_downsample = None\n",
        "        \n",
        "        if stride != 1:\n",
        "            identity_downsample = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0,bias=False),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        return nn.Sequential(MyBlock(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride),\n",
        "            MyBlock(out_channels, out_channels),\n",
        "            nn.Dropout(0.3))\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(x.shape[0], -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "6bCvEnhxoICS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, max_seq_length: int, dim: int, n_layers: int, n_head: int, expansion_factor: int):\n",
        "\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        # P()\n",
        "        self.P_w = nn.Embedding(max_seq_length, dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerBlock(self.dim,self.n_head,self.expansion_factor) for i in range(self.n_layers)])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length*dim)\n",
        "        \"\"\"\n",
        "        batch_size = inputs.size(0)\n",
        "        \n",
        "        x = inputs.reshape(batch_size,self.max_seq_length,self.dim)\n",
        "        p_emb = self.P_w(torch.LongTensor(range(0,self.max_seq_length)).unsqueeze(0).to(inputs.device))\n",
        "        x = x + p_emb  # B,L,D\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,x,x)\n",
        "        \n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3-zQ0MWcmxNU"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Modelo Completo"
      ],
      "metadata": {
        "id": "fjbq3C8UmcnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size: int, \n",
        "        max_seq_length: int, \n",
        "        dim: int, \n",
        "        n_layers: int, \n",
        "        pad_token_id: int, \n",
        "        n_head: int, \n",
        "        expansion_factor: int, \n",
        "        img_channels: int,\n",
        "        resnet_state_dict = None,\n",
        "        decoder_state_dict = None,\n",
        "    ):\n",
        "        super(LanguageModel,self).__init__()         \n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.img_channels = img_channels\n",
        "        self.n_input_enc = self.dim * self.max_seq_length\n",
        "\n",
        "        self.decoder = TransformerDecoder(self.vocab_size, self.max_seq_length, self.dim, self.n_layers, self.pad_token_id, self.n_head, self.expansion_factor)\n",
        "        self.encoder = TransformerEncoder(self.max_seq_length, self.dim, self.n_layers, self.n_head, self.expansion_factor)\n",
        "\n",
        "        self.resnet = CNN_resnet(self.img_channels, 100)\n",
        "\n",
        "        if resnet_state_dict is not None:\n",
        "            self.resnet.load_state_dict(resnet_state_dict)\n",
        "        if decoder_state_dict is not None:\n",
        "            self.decoder.load_state_dict(decoder_state_dict)\n",
        "\n",
        "\n",
        "        self.resnet.fc = nn.Linear(512, self.n_input_enc)  \n",
        "  \n",
        "    def forward(self, input_img, input_text):\n",
        "\n",
        "        enc = self.resnet(input_img)\n",
        "        enc_out = self.encoder(enc)\n",
        "        out = self.decoder(input_text,enc_out=enc_out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "tT1GLec4S4iu"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17546236-9d59-482d-ed5f-da7139d82449"
      },
      "source": [
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=256,\n",
        "    n_layers=4,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    n_head = 4,\n",
        "    expansion_factor = 4,\n",
        "    img_channels = 3\n",
        ").to(device)\n",
        "\n",
        "sample_img, sample_label = next(iter(DataLoader(training_dataset)))\n",
        "sample_img = sample_img.to(device)\n",
        "sample_input = sample_label.to(device)\n",
        "\n",
        "sample_img = sample_img.unsqueeze(dim=1).expand(-1,sample_input.shape[1],-1,-1,-1).reshape(-1,sample_img.shape[-3],sample_img.shape[-2],sample_img.shape[-1])\n",
        "sample_input = sample_input.reshape(-1,sample_label.shape[-1])\n",
        "\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_img.shape: {sample_img.shape}')\n",
        "\n",
        "\n",
        "sample_output = model(sample_img, sample_input[:,:-1])\n",
        "print(f'sample_input.shape: {sample_input[:,:-1].shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([5, 13])\n",
            "sample_img.shape: torch.Size([5, 3, 480, 480])\n",
            "sample_input.shape: torch.Size([5, 12])\n",
            "sample_output.shape: torch.Size([5, 12, 28996])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_img, sample_label = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_label.to(device)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_img.shape: {sample_img.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anXk_J70zsjg",
        "outputId": "4b2a0726-25a5-4326-b4e3-a4b63a871fd7"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([1, 5, 13])\n",
            "sample_img.shape: torch.Size([1, 3, 480, 480])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6003235-13ce-4405-8c4e-45d6a3313585"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 34994116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 64\n",
        "\n",
        "train_imag, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_imag = train_imag.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "# train_imag = train_imag.unsqueeze(dim=1).expand(-1,sample_input.shape[1],-1,-1,-1).reshape(-1,sample_img.shape[-3],sample_img.shape[-2],sample_img.shape[-1])\n",
        "# train_target_ids = train_target_ids.reshape(-1,sample_label.shape[-1])\n",
        "\n",
        "\n",
        "print(train_imag.shape)\n",
        "print(train_target_ids.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a21fa6-176a-443f-f63b-e47c77e725f6"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 100, 100])\n",
            "torch.Size([64, 5, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logitos_all = torch.zeros(train_target_ids.shape[0],train_target_ids.shape[1],(train_target_ids.shape[2]-1),tokenizer.vocab_size).to(device)\n",
        "print(logitos_all.shape)\n",
        "for i in range(5):\n",
        "    target = train_target_ids[:,i,:].reshape(-1,train_target_ids.shape[-1])\n",
        "    logits = model(train_imag,target[:,:-1])  \n",
        "    logitos_all[:,i,:]=  logits\n",
        "\n",
        "print(logitos_all.shape)\n",
        "my_perplexity = perplexity(logits=logitos_all, target=train_target_ids[:,:,1:], ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwFqaXfNLTKy",
        "outputId": "59b67073-279c-42d3-d35f-9d24a034c5c3"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 5, 12, 28996])\n",
            "torch.Size([64, 5, 12, 28996])\n",
            "my perplexity:              35216\n",
            "correct initial perplexity: 28996\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'model_completo_02.pt'\n",
        "path_modelo = F\"/content/drive/My Drive/modelo_proyecto/{model_save_name}\"\n",
        "\n",
        "model_resnet = 'model_resnet18_CIFAR_01.pt'\n",
        "model_decoder = 'model_decoder_bookcorpus_dataset_separado.pt'\n",
        "\n",
        "path_resnet = F\"/content/drive/My Drive/modelo_proyecto/pre_trained/{model_resnet}\"\n",
        "path_decoder = F\"/content/drive/My Drive/modelo_proyecto/pre_trained/{model_decoder}\""
      ],
      "metadata": {
        "id": "lyb-He4AFHFP"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "0adf9cb3-3c11-46b8-cbaf-6aa2bb344c34"
      },
      "source": [
        "max_examples = 150_000_000\n",
        "eval_every_steps = 100\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "dim = 256\n",
        "n_layers = 4\n",
        "n_head = 4\n",
        "expansion_factor = 4\n",
        "img_channels = 3\n",
        "\n",
        "checkpoint_resnet = torch.load(path_resnet)\n",
        "checkpoint_decoder = torch.load(path_decoder)\n",
        "\n",
        "\n",
        "model = LanguageModel(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=dim,\n",
        "    n_layers=n_layers,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    n_head = n_head,\n",
        "    expansion_factor = expansion_factor,\n",
        "    img_channels = 3,\n",
        "    resnet_state_dict = checkpoint_resnet['model_state_dict'],\n",
        "    decoder_state_dict = checkpoint_decoder['model_state_dict'],\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=16)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "checkpoint = {'vocab_size': tokenizer.vocab_size,\n",
        "              'max_seq_length': max_seq_length,\n",
        "              'dim': dim,\n",
        "              'n_layers': n_layers,\n",
        "              'pad_token_id': tokenizer.pad_token_id,\n",
        "              'n_head': n_head,\n",
        "              'expansion_factor': expansion_factor,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict' : optimizer.state_dict(),\n",
        "              'step': [],\n",
        "              'n_examples': [],\n",
        "              'train_ppl': [],\n",
        "              'valid_ppl': []\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, path_modelo)\n",
        "\n",
        "def train_step(input_img, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logitos_all = torch.zeros(target_ids.shape[0],target_ids.shape[1],(target_ids.shape[2]-1),tokenizer.vocab_size).to(device)\n",
        "    for i in range(5):\n",
        "        target = target_ids[:,i,:].reshape(-1,target_ids.shape[-1])\n",
        "        logits = model(input_img,target[:,:-1])  \n",
        "        logitos_all[:,i,:]=  logits\n",
        "\n",
        "    logitos_all = logitos_all.reshape(-1, logitos_all.shape[-1])\n",
        "    target_ids = target_ids[:,:,1:].reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logitos_all, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_img, target_ids):\n",
        "    model.eval()\n",
        "    logitos_all = torch.zeros(target_ids.shape[0],target_ids.shape[1],(target_ids.shape[2]-1),tokenizer.vocab_size).to(device)\n",
        "    for i in range(5):\n",
        "        target = target_ids[:,i,:].reshape(-1,target_ids.shape[-1])\n",
        "        logits = model(input_img,target[:,:-1])  \n",
        "        logitos_all[:,i,:]=  logits\n",
        "\n",
        "    logitos_all = logitos_all.reshape(-1, logitos_all.shape[-1])\n",
        "    target_ids = target_ids[:,:,1:].reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logitos_all, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_image, train_output in train_loader:\n",
        "        loss = train_step(train_image.to(device), train_output.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "            checkpoint['step'].append(n_examples)\n",
        "            checkpoint['n_examples'].append(n_examples)\n",
        "            checkpoint['train_ppl'].append(train_ppl)\n",
        "            checkpoint['valid_ppl'].append(valid_ppl)\n",
        "\n",
        "            checkpoint['model_state_dict'] = model.state_dict()\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "            torch.save(checkpoint, path_modelo)\n",
        "\n",
        "        n_examples += len(train_image)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 10741.83, valid ppl: 4355.72\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-175-81fbfb1c82f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mn_examples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-175-81fbfb1c82f9>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_img, target_ids)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogitos_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint['model_state_dict'] = model.state_dict()\n",
        "checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "torch.save(checkpoint, path_modelo)"
      ],
      "metadata": {
        "id": "mWl_Mn3WerWS"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'model_completo.pt'\n",
        "path_modelo = F\"/content/drive/My Drive/modelo_proyecto/{model_save_name}\""
      ],
      "metadata": {
        "id": "FQTpy71bt1Hk"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7971b32-5f2c-4346-f649-6ba60852ed95",
        "id": "pGLLmAWybn9K"
      },
      "source": [
        "# seguir entrenando modelo guardado\n",
        "max_examples = 150_000_000\n",
        "eval_every_steps = 10\n",
        "\n",
        "lr = 3e-4\n",
        "\n",
        "checkpoint = torch.load(path_modelo)\n",
        "model = LanguageModel(\n",
        "    vocab_size = checkpoint['vocab_size'],\n",
        "    max_seq_length = checkpoint['max_seq_length'],\n",
        "    dim = checkpoint['dim'],\n",
        "    n_layers = checkpoint['n_layers'],\n",
        "    pad_token_id = checkpoint['pad_token_id'],\n",
        "    n_head = checkpoint['n_head'],\n",
        "    expansion_factor = checkpoint['expansion_factor'],\n",
        "    img_channels = 3,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=16)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "def train_step(input_img, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "\n",
        "    logitos_all = torch.zeros(target_ids.shape[0],target_ids.shape[1],(target_ids.shape[2]-1),tokenizer.vocab_size).to(device)\n",
        "    for i in range(5):\n",
        "        target = target_ids[:,i,:].reshape(-1,target_ids.shape[-1])\n",
        "        logits = model(input_img,target[:,:-1])  \n",
        "        logitos_all[:,i,:]=  logits\n",
        "\n",
        "    logitos_all = logitos_all.reshape(-1, logitos_all.shape[-1])\n",
        "    target_ids = target_ids[:,:,1:].reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logitos_all, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_img, target_ids):\n",
        "    model.eval()\n",
        "    logitos_all = torch.zeros(target_ids.shape[0],target_ids.shape[1],(target_ids.shape[2]-1),tokenizer.vocab_size).to(device)\n",
        "    for i in range(5):\n",
        "        target = target_ids[:,i,:].reshape(-1,target_ids.shape[-1])\n",
        "        logits = model(input_img,target[:,:-1])  \n",
        "        logitos_all[:,i,:]=  logits\n",
        "\n",
        "    logitos_all = logitos_all.reshape(-1, logitos_all.shape[-1])\n",
        "    target_ids = target_ids[:,:,1:].reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logitos_all, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for s in zip(checkpoint['step'], checkpoint['n_examples'], checkpoint['train_ppl'], checkpoint['valid_ppl']):\n",
        "    print(f'{s[0]} steps; {s[1]} examples so far; train ppl: {s[2]:.2f}, valid ppl: {s[3]:.2f}')\n",
        "\n",
        "n_examples = checkpoint['n_examples'][-1]\n",
        "step = checkpoint['step'][-1]\n",
        "\n",
        "while n_examples < max_examples:\n",
        "    for train_image, train_output in train_loader:\n",
        "        loss = train_step(train_image.to(device), train_output.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "            checkpoint['step'].append(n_examples)\n",
        "            checkpoint['n_examples'].append(n_examples)\n",
        "            checkpoint['train_ppl'].append(train_ppl)\n",
        "            checkpoint['valid_ppl'].append(valid_ppl)\n",
        "\n",
        "            checkpoint['model_state_dict'] = model.state_dict()\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "            torch.save(checkpoint, path_modelo)\n",
        "\n",
        "        n_examples += len(train_image)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 10741.83, valid ppl: 4355.72\n",
            "0 steps; 0 examples so far; train ppl: 32.85, valid ppl: 29.42\n",
            "0 steps; 0 examples so far; train ppl: 37.72, valid ppl: 29.38\n",
            "10 steps; 160 examples so far; train ppl: 31.67, valid ppl: 29.54\n",
            "20 steps; 320 examples so far; train ppl: 31.69, valid ppl: 29.39\n",
            "30 steps; 480 examples so far; train ppl: 33.31, valid ppl: 29.48\n",
            "40 steps; 640 examples so far; train ppl: 31.80, valid ppl: 29.68\n",
            "50 steps; 800 examples so far; train ppl: 36.24, valid ppl: 29.48\n",
            "60 steps; 960 examples so far; train ppl: 33.78, valid ppl: 29.51\n",
            "70 steps; 1120 examples so far; train ppl: 30.53, valid ppl: 29.46\n",
            "80 steps; 1280 examples so far; train ppl: 29.72, valid ppl: 29.26\n",
            "90 steps; 1440 examples so far; train ppl: 32.65, valid ppl: 29.28\n",
            "100 steps; 1600 examples so far; train ppl: 29.42, valid ppl: 29.00\n",
            "110 steps; 1760 examples so far; train ppl: 30.03, valid ppl: 29.37\n",
            "120 steps; 1920 examples so far; train ppl: 31.37, valid ppl: 29.49\n",
            "130 steps; 2080 examples so far; train ppl: 32.91, valid ppl: 29.31\n",
            "140 steps; 2240 examples so far; train ppl: 32.07, valid ppl: 28.99\n",
            "150 steps; 2400 examples so far; train ppl: 32.31, valid ppl: 29.29\n",
            "160 steps; 2560 examples so far; train ppl: 33.87, valid ppl: 29.19\n",
            "170 steps; 2720 examples so far; train ppl: 37.47, valid ppl: 29.09\n",
            "180 steps; 2880 examples so far; train ppl: 34.97, valid ppl: 29.39\n",
            "190 steps; 3040 examples so far; train ppl: 32.30, valid ppl: 29.19\n",
            "200 steps; 3200 examples so far; train ppl: 37.13, valid ppl: 28.97\n",
            "210 steps; 3360 examples so far; train ppl: 31.17, valid ppl: 29.21\n",
            "220 steps; 3520 examples so far; train ppl: 32.52, valid ppl: 28.87\n",
            "230 steps; 3680 examples so far; train ppl: 36.00, valid ppl: 29.09\n",
            "240 steps; 3840 examples so far; train ppl: 35.29, valid ppl: 29.02\n",
            "250 steps; 4000 examples so far; train ppl: 34.00, valid ppl: 29.01\n",
            "260 steps; 4160 examples so far; train ppl: 32.76, valid ppl: 29.26\n",
            "270 steps; 4320 examples so far; train ppl: 32.45, valid ppl: 29.10\n",
            "280 steps; 4480 examples so far; train ppl: 31.65, valid ppl: 29.11\n",
            "290 steps; 4640 examples so far; train ppl: 32.20, valid ppl: 29.17\n",
            "300 steps; 4800 examples so far; train ppl: 32.50, valid ppl: 29.11\n",
            "310 steps; 4960 examples so far; train ppl: 35.58, valid ppl: 28.84\n",
            "320 steps; 5120 examples so far; train ppl: 30.23, valid ppl: 28.83\n",
            "330 steps; 5280 examples so far; train ppl: 33.46, valid ppl: 28.95\n",
            "340 steps; 5440 examples so far; train ppl: 32.53, valid ppl: 28.64\n",
            "350 steps; 5600 examples so far; train ppl: 29.52, valid ppl: 29.02\n",
            "360 steps; 5760 examples so far; train ppl: 31.10, valid ppl: 28.75\n",
            "370 steps; 5920 examples so far; train ppl: 33.08, valid ppl: 28.82\n",
            "380 steps; 6080 examples so far; train ppl: 33.76, valid ppl: 28.77\n",
            "390 steps; 6240 examples so far; train ppl: 29.84, valid ppl: 28.76\n",
            "400 steps; 6400 examples so far; train ppl: 32.57, valid ppl: 28.82\n",
            "410 steps; 6560 examples so far; train ppl: 34.57, valid ppl: 28.83\n",
            "420 steps; 6720 examples so far; train ppl: 33.19, valid ppl: 28.54\n",
            "430 steps; 6880 examples so far; train ppl: 31.98, valid ppl: 28.89\n",
            "440 steps; 7040 examples so far; train ppl: 33.30, valid ppl: 28.62\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-b333fed133b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                 valid_ppl = np.exp(np.average([\n\u001b[1;32m     79\u001b[0m                     \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_target_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     for val_input_ids, val_target_ids in validation_loader]))\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-179-b333fed133b5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 valid_ppl = np.exp(np.average([\n\u001b[1;32m     79\u001b[0m                     \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_target_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     for val_input_ids, val_target_ids in validation_loader]))\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-179-b333fed133b5>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(input_img, target_ids)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mlogitos_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96a162c-8f0d-4c00-f869-a6ba7c57e4dc"
      },
      "source": [
        "test_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 28.740761493303268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_dataset = torchvision.datasets.CocoCaptions(root = path2data_val, annFile = path2capt_val)\n",
        "\n",
        "img_test, label_test = next(iter(test_dataset))\n",
        "label_init=[]\n",
        "fig=plt\n"
      ],
      "metadata": {
        "id": "AvnNSXpcr2Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A man is eating pizza with his girlfriend and his dog in'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c86e73f-9e7c-4554-e535-bd899c99b727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man is eating pizza with his girlfriend and his dog in the\n",
            "A man is eating pizza with his girlfriend and his dog in the job\n",
            "A man is eating pizza with his girlfriend and his dog in the job of\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the r\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob.\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. he\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of the\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of the doctor\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of the doctor,\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of the doctor, but\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of the doctor, but he\n",
            "A man is eating pizza with his girlfriend and his dog in the job of the rob. hes the most important thriller of the doctor, but hes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The young dog is sleeping in the park and playing with a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a95a8b-d54e-4f88-b64c-4e4314e775b6",
        "id": "EwHpxq6zltEC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The young dog is sleeping in the park and playing with a picture\n",
            "The young dog is sleeping in the park and playing with a picture.\n",
            "The young dog is sleeping in the park and playing with a picture. the\n",
            "The young dog is sleeping in the park and playing with a picture. the group\n",
            "The young dog is sleeping in the park and playing with a picture. the group of\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows.\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows. the\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows. the horses\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows. the horses were\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows. the horses were streaks\n",
            "The young dog is sleeping in the park and playing with a picture. the group of the horses on the men were everywhere and shadows. the horses were streaks and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The old dog is running in the park and playing with a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY8iKefm8IiU",
        "outputId": "30095bfd-09ac-4c1b-86ed-3237f0e86948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The old dog is running in the park and playing with a f\n",
            "The old dog is running in the park and playing with a fris\n",
            "The old dog is running in the park and playing with a frisbee\n",
            "The old dog is running in the park and playing with a frisbee.\n",
            "The old dog is running in the park and playing with a frisbee. A\n",
            "The old dog is running in the park and playing with a frisbee. A man\n",
            "The old dog is running in the park and playing with a frisbee. A man in\n",
            "The old dog is running in the park and playing with a frisbee. A man in a\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing f\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing fris\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing frisbee\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing frisbee game\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing frisbee game.\n",
            "The old dog is running in the park and playing with a frisbee. A man in a black shirt and white shorts playing frisbee game. A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}